{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Machine Learning Predictor for Airfoil Self-Noise using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are a data engineer at an aeronautics consulting company. Your company prides itself in being able to efficiently design airfoils for use in planes and sports cars. Data scientists in your office need to work with different algorithms and data in different formats. While they are good at Machine Learning, they count on you to be able to do ETL jobs and build ML pipelines. In this project you will use the modified version of the NASA Airfoil Self Noise dataset. You will clean this dataset, by dropping the duplicate rows, and removing the rows with null values. You will create an ML pipe line to create a model that will predict the SoundLevel based on all the other columns. You will evaluate the model and towards the end you will persist the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II Create a  Machine Learning Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Airfoil with flow](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/images/Airfoil_with_flow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Read processed data from S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1  Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code goes here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.ml.regression import LinearRegression,RandomForestRegressor\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, StringIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark==3.1.2 -q\n",
    "!pip install findspark -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# FindSpark simplifies the process of using Apache Spark with Python\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Initialising Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NASA_Project-01\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"YOUR_ACCESS_KEY\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"YOUR_SECRET_KEY\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 - Load data from \"NASA_airfoil_noise_cleaned.parquet\" into a dataframe from S3 bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------+------------------+-----------------------+------------------+\n",
      "|Frequency|AngleOfAttack|ChordLength|FreeStreamVelocity|SuctionSideDisplacement|SoundLevelDecibels|\n",
      "+---------+-------------+-----------+------------------+-----------------------+------------------+\n",
      "|      630|          0.0|     0.3048|              31.7|             0.00331266|           129.095|\n",
      "|     4000|          0.0|     0.3048|              31.7|             0.00331266|           118.145|\n",
      "|     4000|          1.5|     0.3048|              39.6|             0.00392107|           117.741|\n",
      "|      800|          4.0|     0.3048|              71.3|             0.00497773|           131.755|\n",
      "|     1250|          0.0|     0.2286|              31.7|              0.0027238|           128.805|\n",
      "+---------+-------------+-----------+------------------+-----------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Read parquet file\n",
    "\n",
    "df = spark.read.parquet(\"s3a://YOUR_BUCKET_NAME/PATH/nasa_airfoil_noise_raw.parquet\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2 - Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be no cleaning of the data, as it has been already cleaned from source. Instead, the focus will be on understanding the data and optimising its use to build a good ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1 - Data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important aspects to look for are missing values, data distribution and possible outliers. Making a preliminary data preview and providing a statistical description can offer an initial understanding of the data that will be further developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rowcount =df.count()\n",
    "print(rowcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas().isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***There are no missing values in the data, as expected.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 -  Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a - Histograms and distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create a histogram for each variable to see their distribution\n",
    "fig, ax = plt.subplots(2,3, figsize=(15,10))\n",
    "ax[0][0].set_xscale(\"log\") # This command sets the X-axis of \"Frequency\" in log scale\n",
    "ax[1][1].set_xscale(\"log\") # This command sets the X-axis of \"SuctionSideDisplacement\" in log scale\n",
    "\n",
    "for i in range(0,2): # rows\n",
    "    for j in range(0,3): # columns\n",
    "        col = df.columns[j+3*i]\n",
    "        sns.histplot(df.toPandas(), x=col, ax=ax[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the target variable \"SoundLevel\" is slightly skewed, so it could be transformed to be \"more normal\".Note on x scales: Frequencies are always measured in a log scale, so that was directly set so; while SuctionSideDisplacement was set in a log scale by pure observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b - Scatterplots and correlation with Sound Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3, figsize=(15,10))\n",
    "ax[0][0].set_xscale(\"log\") # This command sets the X-axis of \"Frequency\" in log scale\n",
    "ax[1][1].set_xscale(\"log\") # This command sets the X-axis of \"SuctionSideDisplacement\" in log scale\n",
    "for i in range(0,2): # rows\n",
    "    for j in range(0,3): # columns\n",
    "        col = df.columns[3*i+j]\n",
    "        sns.scatterplot(df.toPandas(), x=col, y='SoundLevel', ax=ax[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, there is no clear linear dependency between target and independent variables, which suggests studying feature transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c - Interdependency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a correlation matrix to highlight connections between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.toPandas().corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The correlation matrix illustrates the degree of intercorrelation among independent variables. Notably, it reveals a particularly high correlation coefficient between 'SuctionSideDisplacement' and 'AngleOfAttack'. This significant correlation suggests a strong relationship between these features, potentially indicating shared or dependent information. It is highly likely that a polynomial feature expansion will help improve the performance of the predictor, given this relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Feature Extraction and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above Exploratory Data Analysis, the selected transformations are:\n",
    "* Log transformation\n",
    "* Polynomial expansion\n",
    "* Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 - Log transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While inspecting the distributions of 'Frequency' and 'SuctionSideDisplacement', both exhibit noticeable skewness, deviating from the expected bell-shaped curve. These skewed distributions have the potential to influence model performance. It will be employed a log transformation to mitigate the skewness, aiming to align these variables with a more symmetric distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 - Polynomial expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inclusion of polynomial features facilitates the representation of higher-order interactions, which are hinted by the above's correlation matrix. This expansion enlarges the feature space, givig the predictor a more comprehensive understanding of the underlying relationships, and strongly likely elevating its predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 - Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the substantial scale variations among variables, particularly accentuated within the polynomial terms, standardizing the data emerges as a pivotal step. This transformation ensures a consistent scale across all features, mitigating the disparate magnitudes and leading to improved model convergence and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4 - Build the pipelines for each ML algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this section, the pipelines will be defined for every algorithm and two variants each, one with data transformations applied, the other without them. This dual-pipeline approach serves as a strategic methodology to investigate the influence of feature engineering on the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Pipeline Flow with data transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flow of the pipeline is defined in 4 stages,\n",
    "\n",
    "* Stage 1.- Assemble the independent variables into one single column called \"Features\"\n",
    "* Stage 2.- Create the polinomial feature columns\n",
    "* Stage 3.- Standardise the data with StandardScaler\n",
    "* Stage 4.- Feed data into the regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.  Pipeline Flow without data transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Much simpler flow this time, with only 2 stages,\n",
    "\n",
    "* Stage 1.- Assemble the independent variables into one single column called \"Features\"\n",
    "* Stage 2.- Feed data into the regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 - Pipeline for the Unregularised Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a - Pipeline Flow with data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformer to apply the log transform to Frequency and SuctionSideDisplacement\n",
    "log_transform_sql = SQLTransformer(\n",
    "    statement=\"SELECT log(frequency + 1) AS log_frequency, AngleOfAttack, ChordLength, FreeStreamVelocity, log(SuctionSideDisplacement + 1) AS log_SSD, SoundLevel FROM __THIS__\"\n",
    ")\n",
    "\n",
    "# Create an assembler\n",
    "assembler = VectorAssembler(inputCols=('log_frequency', 'AngleOfAttack', 'ChordLength', 'FreeStreamVelocity', 'log_SSD'), outputCol='features')\n",
    "\n",
    "# Create a polynomial expansion transformer\n",
    "from pyspark.ml.feature import PolynomialExpansion\n",
    "px = PolynomialExpansion(degree=2)\n",
    "px.setInputCol(\"features\")\n",
    "px.setOutputCol(\"PolyFeat\")\n",
    "\n",
    "# Create an standard scaler\n",
    "scaler = StandardScaler(inputCol=\"PolyFeat\", outputCol=\"finalFeatures\")\n",
    "\n",
    "# Finally, create the unregularised linear regressor (regParam=0 indicates no regularisation terms)\n",
    "lru = LinearRegression(featuresCol=\"finalFeatures\", labelCol=\"SoundLevel\", predictionCol=\"PredictedSoundLevel\", regParam=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Consolidate the pipeline with spark's Pipeline function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[log_transform_sql, assembler, px, scaler,lru])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. -  Without data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new assembler with the untransformed columns\n",
    "assemblerWO = VectorAssembler(inputCols=df.drop(\"SoundLevel\").columns, outputCol='finalFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineWO = Pipeline(stages=[assemblerWO, lru])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 - Pipeline for the Regularised Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a - Pipeline Flow with data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since all the other elements of the pipeline can be reused, it is only needed a new linear regressor (regParam=1 indicates the weight of the regularisation term)\n",
    "lrr = LinearRegression(featuresCol=\"finalFeatures\", labelCol=\"SoundLevel\", predictionCol=\"PredictedSoundLevel\", regParam=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineReg = Pipeline(stages=[log_transform_sql, assembler, px, scaler,lrr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. -  Without data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineRegWO = Pipeline(stages=[assemblerWO, lrr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ####  4.3  Pipeline for the Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### a - Pipeline Flow with data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(featuresCol='finalFeatures', labelCol='SoundLevel', predictionCol='PredictedSoundLevel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineRF = Pipeline(stages=[log_transform_sql, assembler, px, scaler,rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. -  Without data transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineRFWO = Pipeline(stages=[assemblerWO, rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 5.  Evaluate the different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Preparing for the evaluation\n",
    "\n",
    "Before starting, it is necessary to split the dataset into training and validation parts, create the grid search as well, and defining a common evaluator for all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing with 30% of data saved for validation\n",
    "(trainingData, testingData) = df.randomSplit([0.7, 0.3], seed=123)\n",
    "\n",
    "# Initialise an evaluator (RMSE for regression)\n",
    "evaluator = RegressionEvaluator(labelCol='SoundLevel', predictionCol='PredictedSoundLevel', metricName='rmse')\n",
    "\n",
    "# Predctions graph function\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "def plotPred(pred,predWO, model='Regression'):\n",
    "    '''Plots the comparison between predictions and actual values.'''\n",
    "\n",
    "    # Compute residuals\n",
    "    residDF = pred.withColumn(\"Residuals\", col('SoundLevel')- col('PredictedSoundLevel'))\n",
    "    residDFWO = predWO.withColumn(\"Residuals\", col('SoundLevel')- col('PredictedSoundLevel'))\n",
    "\n",
    "    # Convert Spark DFs to Pandas so Seaborn can handle them\n",
    "    resid_df = residDF.toPandas()\n",
    "    resid_dfWO = residDFWO.toPandas()\n",
    "\n",
    "    # Compute lowess curve\n",
    "    lowess_vals = lowess(resid_df.Residuals.ravel(), resid_df.SoundLevel.ravel(), frac=0.5, it=10)\n",
    "    lowess_valsWO = lowess(resid_dfWO.Residuals.ravel(), resid_dfWO.SoundLevel.ravel(), frac=0.5, it=10)\n",
    "\n",
    "    # Create the grid of plots\n",
    "    fig, ax = plt.subplots(1,2, figsize=(15,6))\n",
    "    \n",
    "    # Plot the data points\n",
    "    ax[0].plot([70,140], [0,0], ls='--', color='gray')\n",
    "    ax[1].plot([70,140], [0,0], ls='--', color='gray')\n",
    "    \n",
    "    sns.scatterplot(resid_df, x=\"SoundLevel\", y=\"Residuals\", ax=ax[0], label='residuals')\n",
    "    sns.scatterplot(resid_dfWO, x=\"SoundLevel\", y=\"Residuals\", ax=ax[1], label='residuals')\n",
    "\n",
    "    ax[0].plot(lowess_vals[:, 0], lowess_vals[:, 1], color='red', label='lowess')\n",
    "    ax[1].plot(lowess_valsWO[:, 0], lowess_valsWO[:, 1], color='red', label='lowess')\n",
    "\n",
    "    \n",
    "\n",
    "    # Set plot labels and title\n",
    "    ax[0].set_title(\"SoundLevel prediction with DT residuals\")\n",
    "    ax[0].set_ylabel(\"Residuals\")\n",
    "    ax[0].set_xlabel(\"Real Sound Level values (dB)\")\n",
    "    ax[0].set_ylim((-15,15))\n",
    "    ax[0].set_xlim((100,138))\n",
    "    ax[0].legend(loc='upper left')    \n",
    "    \n",
    "    ax[1].set_title(\"SoundLevel prediction without DT residuals\")\n",
    "    ax[1].set_ylabel(\"Residuals\")\n",
    "    ax[1].set_xlabel(\"Real Sound Level values (dB)\")\n",
    "    ax[1].set_ylim((-15,15))\n",
    "    ax[1].set_xlim((100,138))\n",
    "    ax[1].legend(loc='upper left')\n",
    "    \n",
    "    fig.suptitle(f\"Comparison between residuals of {model}\", fontweight='bold')\n",
    "\n",
    "    # Show the plot\n",
    "    # fig.show() # I have an error where it's plotting two times, so I commented this in order to plot only once\n",
    "\n",
    "def plotComparison(pred1, pred2, model1='model 1', model2='model 2'):\n",
    "    \n",
    "    # Compute residuals\n",
    "    resid1 = pred1.withColumn(\"Residuals\", -col('SoundLevel')+ col('PredictedSoundLevel'))\n",
    "    resid2 = pred2.withColumn(\"Residuals\", -col('SoundLevel')+ col('PredictedSoundLevel'))\n",
    "\n",
    "    # Convert Spark DFs to Pandas so Seaborn can handle them\n",
    "    resid_1 = resid1.toPandas()\n",
    "    resid_2 = resid2.toPandas()\n",
    "\n",
    "    # Compute lowess curve\n",
    "    lowess_vals1 = lowess(resid_1.Residuals.ravel(), resid_1.SoundLevel.ravel(), frac=0.5, it=10)\n",
    "    lowess_vals2 = lowess(resid_2.Residuals.ravel(), resid_2.SoundLevel.ravel(), frac=0.5, it=10)\n",
    "\n",
    "    # Create the grid of plots\n",
    "    fig, ax = plt.subplots(1,2, figsize=(15,6))\n",
    "    \n",
    "    # Plot the data points\n",
    "    ax[0].plot([70,140], [0,0], ls='--', color='gray')\n",
    "    ax[1].plot([70,140], [0,0], ls='--', color='gray')\n",
    "    \n",
    "    sns.scatterplot(resid_1, x=\"SoundLevel\", y=\"Residuals\", ax=ax[0], label='residuals')\n",
    "    sns.scatterplot(resid_2, x=\"SoundLevel\", y=\"Residuals\", ax=ax[1], label='residuals')\n",
    "\n",
    "    ax[0].plot(lowess_vals1[:, 0], lowess_vals1[:, 1], color='red', label='lowess')\n",
    "    ax[1].plot(lowess_vals2[:, 0], lowess_vals2[:, 1], color='red', label='lowess')\n",
    "\n",
    "    # Set plot labels and title\n",
    "    ax[0].set_title(f\"Prediction's residual of {model1}\")\n",
    "    ax[0].set_ylabel(\"Negative Residuals\")\n",
    "    ax[0].set_xlabel(\"Real Sound Level values (dB)\")\n",
    "    ax[0].set_ylim((-15,15))\n",
    "    ax[0].set_xlim((100,138))\n",
    "    ax[0].legend(loc='upper left')\n",
    "    \n",
    "    ax[1].set_title(f\"Prediction's residual of {model2}\")\n",
    "    ax[1].set_ylabel(\"Negative Residuals\")\n",
    "    ax[1].set_xlabel(\"Real Sound Level values (dB)\")\n",
    "    ax[1].set_ylim((-15,15))\n",
    "    ax[1].set_xlim((100,138))\n",
    "    ax[1].legend(loc='upper left')\n",
    "\n",
    "    fig.suptitle(f\"Residuals comparison between {model1} and {model2}\", fontweight='bold')\n",
    "\n",
    "    # Show the plot\n",
    "    # fig.show() # I have an error where it's plotting two times, so I commented this in order to plot only once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 5.1 - Unregularised Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pipeline with data transformations'''\n",
    "# Fit the model\n",
    "lruModel = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions\n",
    "lruPredictions = lruModel.transform(testingData)\n",
    "\n",
    "'''Pipeline without data transformations'''\n",
    "# Fit the model\n",
    "lruModelWO = pipelineWO.fit(trainingData)\n",
    "\n",
    "# Make predictions\n",
    "lruPredictionsWO = lruModelWO.transform(testingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPred(lruPredictions, lruPredictionsWO, \"the unregularised linear regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ####  Training set rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cross validation average rmse for trainingData'''\n",
    "\n",
    "# With DT\n",
    "trainingRmseLRU = evaluator.evaluate(lruModel.transform(trainingData))\n",
    "print(f\"With DT \\t ---> Training set rmse: {trainingRmseLRU}\")\n",
    "\n",
    "# Without DT\n",
    "trainingRmseLRUWO = evaluator.evaluate(lruModelWO.transform(trainingData))\n",
    "print(f\"Without DT \\t ---> Training set rmse: {trainingRmseLRUWO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ####  Test set rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cross validation average rmse for trainingData'''\n",
    "\n",
    "# With DT\n",
    "rmseLRU = evaluator.evaluate(lruPredictions) # This value is the average metric for each fold of the cross validation\n",
    "print(f\"With DT \\t ---> Testing set rmse: {rmseLRU}\")\n",
    "\n",
    "# Without DT\n",
    "rmseLRUWO = evaluator.evaluate(lruPredictionsWO) # This value is the average metric for each fold of the cross validation\n",
    "print(f\"Without DT\\t ---> Testing set rmse: {rmseLRUWO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 5. 2 Regularised Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating parameter grid for hyperparameter tuning\n",
    "param_grid = (ParamGridBuilder()\n",
    "                            .addGrid(lrr.regParam, [0.01, 0.1, 1.0, 10]) # L2 Ridge regularisation parameter\n",
    "                            .addGrid(lrr.elasticNetParam, [0.0, 0.5, 1.0]) # L1+L2 ElasticNet regularisation parameter\n",
    "                            .build())\n",
    "\n",
    "'''Pipeline with DT'''\n",
    "\n",
    "# Set up CrossValidator with the pipeline for hyperparameter tuning\n",
    "crossval = CrossValidator(estimator=pipelineReg, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Fit the model\n",
    "cv_model = crossval.fit(trainingData)\n",
    "\n",
    "# Make predictions\n",
    "lrrPredictions = cv_model.transform(testingData)\n",
    "\n",
    "'''Pipeline without DT'''\n",
    "\n",
    "# Set up CrossValidator with the pipeline for hyperparameter tuning\n",
    "crossval = CrossValidator(estimator=pipelineRegWO, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Fit the model\n",
    "cv_modelWO = crossval.fit(trainingData)\n",
    "\n",
    "# Make predictions\n",
    "lrrPredictionsWO = cv_modelWO.transform(testingData)\n",
    "\n",
    "\n",
    "# Get the tuned model\n",
    "tuned_lrr = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPred(lrrPredictions, lrrPredictionsWO, 'the regularised linear regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ####   Training set rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cross validation average rmse for testingData'''\n",
    "\n",
    "# With DT\n",
    "rmseLRR_train = evaluator.evaluate(cv_model.transform(trainingData))\n",
    "print(f\"With DT \\t ---> Training set rmse: {rmseLRR_train}\")\n",
    "\n",
    "# Without DT\n",
    "rmseLRRWO = evaluator.evaluate(cv_modelWO.transform(trainingData))\n",
    "print(f\"Without DT \\t ---> Training set rmse: {rmseLRRWO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ####   Test set rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cross validation average rmse for testingData'''\n",
    "\n",
    "# With DT\n",
    "rmseLRR_test = evaluator.evaluate(lrrPredictions)\n",
    "print(f\"With DT \\t ---> Testing set rmse: {rmseLRR_test}\")\n",
    "\n",
    "# Without DT\n",
    "rmseLRRWO = evaluator.evaluate(lrrPredictionsWO)\n",
    "print(f\"Without DT \\t ---> Testing set rmse: {rmseLRRWO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 5. 3  Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ####  Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a parameter grid for hyperparameter tuning\n",
    "param_grid = (ParamGridBuilder().addGrid(rf.numTrees, [10, 20, 30]).addGrid(rf.maxDepth, [5, 10, 15,20]).build())\n",
    "\n",
    "'''Pipeline with DT'''\n",
    "\n",
    "# Set up CrossValidator with the pipeline, evaluator, and parameter grid for the Random Forest algorithm\n",
    "crossval = CrossValidator(estimator=pipelineRF, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Fit the model\n",
    "cv_model = crossval.fit(trainingData)\n",
    "\n",
    "# Make predictions\n",
    "rfPredictions = cv_model.transform(testingData)\n",
    "\n",
    "'''Pipeline without DT'''\n",
    "\n",
    "# Set up CrossValidator with the pipeline, evaluator, and parameter grid for the Random Forest algorithm\n",
    "crossval = CrossValidator(estimator=pipelineRFWO, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Fit the model\n",
    "cv_modelWO = crossval.fit(trainingData)\n",
    "\n",
    "# Make predictions\n",
    "rfPredictionsWO = cv_modelWO.transform(testingData)\n",
    "\n",
    "# Get best model\n",
    "best_model = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotPred(rfPredictions, rfPredictionsWO, 'the random forest regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ####   Training set rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cross validation average rmse for trainingData'''\n",
    "\n",
    "# With DT\n",
    "rmseRF = evaluator.evaluate(cv_model.transform(trainingData))\n",
    "print(f\"With DT \\t ---> Testing set rmse: {rmseRF}\")\n",
    "\n",
    "# Without DT\n",
    "rmseRFWO = evaluator.evaluate(cv_modelWO.transform(trainingData))\n",
    "print(f\"Without DT \\t ---> Testing set rmse: {rmseRFWO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ####   Test set rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Cross validation average rmse for testingData'''\n",
    "\n",
    "# With DT\n",
    "rmseRF = evaluator.evaluate(rfPredictions)\n",
    "print(f\"With DT \\t ---> Testing set rmse: {rmseRF}\")\n",
    "\n",
    "# Without DT\n",
    "rmseRFWO = evaluator.evaluate(rfPredictionsWO)\n",
    "print(f\"Without DT \\t ---> Testing set rmse: {rmseRFWO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 6 Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Linear Regressor performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the RMSE of the un- and regularised versions of the Linear Regressor, something stands out. The model without regularization performs better. This has an interesting implication, being that the regularization parameter stops the regressor from capturing the apparent complex nature of the dataset. It could be said that the LRR model is underfitting the data, if we compare RMSE scores between the training set and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE score of LRR model (training set): {rmseLRR_train} \\t vs \\t RMSE score of LRU model (training set): {trainingRmseLRU}\")\n",
    "print(f\"RMSE score of LRR model (testing set):  {rmseLRR_test}  \\t vs \\t RMSE score of LRU model (testing set):  {rmseLRU}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute difference of rmse scores between training and testing set's\n",
    "diffLRR = rmseLRR_test - rmseLRR_train\n",
    "diffLRU = rmseLRU - trainingRmseLRU\n",
    "print(f\"The difference between scores is {diffLRR} for the LRR model; whilst the difference between scores is {diffLRU} for the LRU model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Greater differences between training and testing sets are commonly associated with underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to the linear regressor, Random Forests does not surprise with its performance. Even before looking at the RMSE, it was foreseeable this was going to be the best model. Inspecting the prediction's residuals graph, a key feature of this algorithm can be seen. The red line is much flatter compared to both linear regressions, which translates into a better comprehesion of data intricacies by the model. This is a consecuence of how the algorithm works and in fact, the linear regression residuals graphs also reveal why Random Forests is suited for this specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Comparison between residual's graphs\n",
    "plotComparison(lruPredictions, rfPredictions, 'unregularised linear regression', 'random forest regression')\n",
    "# Residuals have negative sign for easier interpretation of the reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Airfoil behavior is known to transition between laminar and turbulent flow regimes under different conditions, and that can be seen in the left graph. The lowess or Locally Weighted Scatterplot Smoothing curve exhibits this pattern, by having two distinct regions. From 100 dB to 120 dB approximately, there is a tendency of growing predictions. From 122 dB onwards, this tendency changes and the model starts shrinking its predictions. In fact, turbulence is often associated with increased aerodynamic noise, which could explain why the region that measures greater sound level values has a tendency to be underestimated more.One possible improvement to the linear regressor could be dividing the model into two parts, one relating to laminar flow and low volumes, the other relating to turbulant flow and great volumes. But this problem is not as noticeable in the random forest, due to its non linearity capabilities and variable importance measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformation impact on model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the study of data transformation impact can be often ignored, due to experience with EDA, in cases where the nature of the data proves to be complex it is advisable to do so. Being fluid dynamics famously challenging and heavily nonlinear, and the dataset analysed here consisting of aerodynamic tests, it was very obvious a detailed analysis was needed. \\Comparing the different RMSE scores between pipelines with data transformations and pipelines without them, it is undeniable the transformations help improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the best predictive model is an easy decision, given how the Random Forest algorithm performs over a traditional linear regression model. The decision was grounded in the understanding that airfoil self-noise generation involves intricate, non-linear interactions among various physical phenomena. The Random Forest's ability to capture non-linear patterns, consider variable importance, and handle complex decision boundaries rendered it a powerful tool for this predictive task. Although there are some areas to improve that performance, like splitting into laminar and turbulent flow, or adding the Reynold's number as a variable, Random Forest has already three times lower RMSE than the base linear regression model, which for the purpose of this project, it is enough.In conclusion, this project provides valuable insights into the physics of airfoil self-noise generation and the predictive capabilities of advanced machine learning techniques. The Random Forest model, trained on the NASA Airfoil Self-Noise dataset, offers a robust framework for understanding and predicting the scaled sound pressure level associated with different airfoil configurations. As we navigate the complex realm of aerodynamics, this exploration lays the foundation for further research and applications in aeronautical engineering and noise reduction strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Stop SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Vianney](https://github.com/fermat01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2024-05-26|0.1|Vianney|Initial Version Created|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2024. All rights reserved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
